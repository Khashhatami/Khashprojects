# In this chapter, we will practise with assosiation rules in R 

install.packages("arules")
library(arules)
install.packages("arulesViz")
library(arulesViz)

#Loading our data 
data(Groceries)

#Remember Summary() function is a good way to start off the second step of data mining (explaratoey phase)
summary(Groceries)

# when defining our support, we can choose targeted numbers. For instance we know that yogurt occurs
# about 10 percent of the time in our Matrix. from the summary we read yogurt = 1372, we also know that
# the we have 9835 rows in total. So dividing 1375/9853 can give us roughly the proper percentage of
# support which we need for our targeted analysis. If we do not define support then R give us the 
#frequency of all columns (all possible items in shopping cart). Compare:

itemFrequencyPlot(Groceries)

itemFrequencyPlot(Groceries, support=0.1)

# if we want to include more items on our plot, that would reqiure to lower the support threshold,
# doing do would squeez all the item names on the shopping carts (the column names) so we cannot
#read them. cex.names argument in itemFrequencyplot() function lowers the font size of the names
#along the x-axis to make for representation of more items on our list.
#let's see what happens if we lower the support to only 0.05 and the cex.names to font size 0.5
itemFrequencyPlot(Groceries, support=0.05, cex.names=0.5 )

####################################################################

# For our data mining purposes, we need a transaction data set rather than a data frame. Groceries is
# a transaction data set as we can see here: 
str(Groceries)

#but what if we have a data frame ? well we coerce into a transaction:

data("AdultUCI")
str(AdultUCI)

# To famalirize ourself with the column names let's use help() function

help(AdultUCI)

# now unto transforming the data frame into transaction:

AdultUCI.t <- AdultUCI

names(AdultUCI.t)

# Here we are doing two things:
#First we are turning all the numeric columns into factors
#This is because when using the a-rules our numbers are not 
#literal and simply show as factors whether or not something did or did not happen 

# Second, as we turn these numeric column into factors, we put those column names with hyphon (-)
# in quotation mark. This way R does not confuse the hypon with subtraction symbol and does minus 
#column against eachother
AdultUCI.t$age <- as.factor(AdultUCI.t$age)
AdultUCI.t$fnlwgt <-as.factor(AdultUCI.t$fnlwgt)
AdultUCI.t$"education-num" <- as.factor(AdultUCI.t$"education-num")
AdultUCI.t$"capital-gain" <- as.factor(AdultUCI.t$"capital-gain")
AdultUCI.t$"capital-loss" <- as.factor(AdultUCI.t$"capital-loss")
AdultUCI.t$"hours-per-week" <- as.factor(AdultUCI.t$"hours-per-week")

# Now that our data has been cleaned up, we turn it into a transation with the use
#of as() function
AdultUCI.trans <- as (AdultUCI.t, "transactions")

#let's plot our result:
itemFrequencyPlot (AdultUCI.trans, support=0.2,cex.names=0.5)

###############################################################################

# Now let's use a priori to evaluate what pairs occur together most often: 
#We set up the apriori() command to use a support of 0.005 (half a percent)
and confidence of 0.5 (50 percent) as the minimums.
apriori(Groceries, parameter=list(support= 0.005, confidence=0.5))

#now let's lower the number of the rules (last time it generated 120 rules) 
#by increasing  our support (the frequency):
ruleset <- apriori(Groceries, parameter = list(support = 0.01,confidence = 0.5))

# Now our algorithm has given 15 rules. We can evluate them now:
summary(ruleset)

# in the begining of our summary we encounter this:
#rule length distribution (lhs + rhs):sizes
#3 
#15 
#Looking through this output, we can see that there are 15 rules in total.
#The line starting with “rule length distribution” shows that all 15 of the
#rules have exactly three elements (counting both the LHS and the RHS).
#we also have another term here called lift;The larger the value of lift, the more interesting
#the rule may be.

# now let's take a look at the actual rules 

inspect(ruleset)

#That is why lift gives higher value to lower frequency rules that might have
#hight confidence and support. That is because high frequency of certain items
#(in the case of Groceries transactions, milk), is associated with it being very common place.
#People purchasing milk is not an interesting insight. highly sought after items
#are known to be valuable so our algorithm tries to give weight to rules that are 
#unique and less known.

# Last time we increased the support threshold, we can evaluate the rules manually. 
#however, when the number of rules generated by the algorithm is high, we have another 
#way of evaluating our a-rules which is visualizing our rules. 
#So this time we are going to lower both support and confidence levels so rules can be generated
#from much lower frequencies. This can help us find intresting patterns that only happen on 
#unique occasions but are insightful 
ruleset <- apriori(Groceries,parameter=list(support=0.005, confidence=0.35))

#now we have 357 rules 

#let's visualize our result: 
plot(ruleset)

#On this plot, the lift is shown by the darkness of a dot that
#appears on the plot. The darker the dot, the closer the lift of that rule is to
#4.0, which appears to be the highest lift value among these 357 rules.  

#The other thing we can see from this plot is that although the support of
#rules ranges from somewhere below 1% all the way up above 7%, all of
#the rules with high lift seem to have support below 1%. On the other hand,
#there are rules with high lift and high confidence, which sounds quite
#positive.

# now let's isooalte interesting rules (high lift rules) 
goodrules <- ruleset[quality(ruleset)$lift > 3.5]

inspect(goodrules)
